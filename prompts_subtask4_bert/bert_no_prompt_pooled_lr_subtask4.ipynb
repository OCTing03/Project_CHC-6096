{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xf3lVTZYhbzA"
   },
   "source": [
    "# Initial Setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ORFXeezn5Og"
   },
   "source": [
    "## (Google Colab use only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3570,
     "status": "ok",
     "timestamp": 1620418927808,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 240
    },
    "id": "YFAQ6IgXn8FK",
    "outputId": "25f6ccd2-93f3-4714-9551-e47ee5916705"
   },
   "outputs": [],
   "source": [
    "# Use Google Colab\n",
    "use_colab = False\n",
    "\n",
    "# Is this notebook running on Colab?\n",
    "# If so, then google.colab package (github.com/googlecolab/colabtools)\n",
    "# should be available in this environment\n",
    "\n",
    "# Previous version used importlib, but we could do the same thing with\n",
    "# just attempting to import google.colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    colab_available = True\n",
    "except:\n",
    "    colab_available = False\n",
    "\n",
    "if use_colab and colab_available:\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # cd to the appropriate working directory under my Google Drive\n",
    "    %cd '/content/drive/My Drive/cs696ds_lexalytics/Ronald Gypsum Prompts'\n",
    "    \n",
    "    # Install packages specified in requirements\n",
    "    !pip install -r requirements.txt\n",
    "\n",
    "    %cd 'prompts_subtask4'\n",
    "    \n",
    "    # List the directory contents\n",
    "    !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgzsHF7Zhbzo"
   },
   "source": [
    "## Experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DUpGBmOJhbzs",
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# We will use the following string ID to identify this particular (training) experiments\n",
    "# in directory paths and other settings\n",
    "experiment_id = 'bert_no_prompt_cls_lr_atsc_laptops_bert-base-uncased'\n",
    "\n",
    "# Random seed\n",
    "random_seed = 696\n",
    "\n",
    "# path to pretrained MLM model folder or the string \"bert-base-uncased\"\n",
    "lm_model_path = 'bert-base-uncased'\n",
    "\n",
    "# Test settings\n",
    "testing_batch_size = 32\n",
    "testing_domain = 'restaurants' # 'laptops', 'restaurants', 'joint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3546,
     "status": "ok",
     "timestamp": 1620418927824,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 240
    },
    "id": "keCSh__SY36i",
    "outputId": "7d83760a-ac16-481c-c9e4-6633f493b37d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment ID: bert_no_prompt_cls_lr_atsc_laptops_bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "print(\"Experiment ID:\", experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYZesqTioMvF"
   },
   "source": [
    "## Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5595,
     "status": "ok",
     "timestamp": 1620418929888,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 240
    },
    "id": "MlK_-DrWhbzb",
    "outputId": "5854fa11-ce1f-49a2-a493-6d6b1fb92423"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "NumPy version: 1.21.2\n",
      "PyTorch version: 1.7.0+cu110\n",
      "Transformers version: 4.3.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import copy\n",
    "import inspect\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import tqdm\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import utils\n",
    "\n",
    "# Random seed settings\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# cuBLAS reproducibility\n",
    "# https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\n",
    "# os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\"\n",
    "# torch.set_deterministic(True)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Print version information\n",
    "print(\"Python version: \" + sys.version)\n",
    "print(\"NumPy version: \" + np.__version__)\n",
    "print(\"PyTorch version: \" + torch.__version__)\n",
    "print(\"Transformers version: \" + transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWuR30eUoTWP"
   },
   "source": [
    "## PyTorch GPU settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5585,
     "status": "ok",
     "timestamp": 1620418929892,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 240
    },
    "id": "PfNlm-ykoSlM",
    "outputId": "9b7cc30c-e6ae-404d-ce4f-b2afbdbec29d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Version: 11.0\n",
      "cuDNN Version: 8004\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4090 D\n",
      "CUDA Capabilities: (8, 9)\n",
      "Number of CUDA devices: 1\n",
      "\n",
      "PyTorch device selected: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "    torch_device = torch.device('cuda')\n",
    "\n",
    "    # Set this to True to make your output immediately reproducible\n",
    "    # Note: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    # Disable 'benchmark' mode: Set this False if you want to measure running times more fairly\n",
    "    # Note: https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Faster Host to GPU copies with page-locked memory\n",
    "    use_pin_memory = True\n",
    "    \n",
    "    # Number of compute devices to be used for training\n",
    "    training_device_count = torch.cuda.device_count()\n",
    "\n",
    "    # CUDA libraries version information\n",
    "    print(\"CUDA Version: \" + str(torch.version.cuda))\n",
    "    print(\"cuDNN Version: \" + str(torch.backends.cudnn.version()))\n",
    "    print(\"CUDA Device Name: \" + str(torch.cuda.get_device_name()))\n",
    "    print(\"CUDA Capabilities: \"+ str(torch.cuda.get_device_capability()))\n",
    "    print(\"Number of CUDA devices: \"+ str(training_device_count))\n",
    "    \n",
    "else:\n",
    "    torch_device = torch.device('cpu')\n",
    "    use_pin_memory = False\n",
    "    \n",
    "    # Number of compute devices to be used for training\n",
    "    training_device_count = 1\n",
    "\n",
    "print()\n",
    "print(\"PyTorch device selected:\", torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayX5VRLfocFk"
   },
   "source": [
    "# Prepare Datasets for Prompt-based Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9LAAJP-hbz7"
   },
   "source": [
    "## Load the SemEval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5575,
     "status": "ok",
     "timestamp": 1620418929895,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 240
    },
    "id": "gpL2uHPUhbz9",
    "outputId": "41504d48-f3c5-4361-a055-e24d3046f9c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset sem_eval2014_task4_dataset (../dataset_cache/sem_eval2014_task4_dataset/SemEval2014Task4Dataset - Subtask 4/0.0.1/537edd3b5fdbdb1f3190419cf0a53a4fab3537bc666f17c8c75fa8d0b554e529)\n"
     ]
    }
   ],
   "source": [
    "# Load semeval for both domains\n",
    "restaurants_dataset = datasets.load_dataset(\n",
    "    os.path.abspath('../dataset_scripts/semeval2014_task4/semeval2014_task4.py'),\n",
    "    name=\"SemEval2014Task4Dataset - Subtask 4\",\n",
    "    data_files={\n",
    "        'test': '../dataset_files/semeval_2014/Restaurants_Test_Gold.xml',\n",
    "        'train': '../dataset_files/semeval_2014/Restaurants_Train_v2.xml',\n",
    "    },\n",
    "    cache_dir='../dataset_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Gi5m8AbPj1iJ"
   },
   "outputs": [],
   "source": [
    "# The dataset chosen for testing\n",
    "if testing_domain == 'restaurants':\n",
    "    test_set = restaurants_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5557,
     "status": "ok",
     "timestamp": 1620418929901,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 240
    },
    "id": "Est9ao9rcH4l",
    "outputId": "8487f46b-593a-43f6-b98c-eb559af8d169"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "973\n"
     ]
    }
   ],
   "source": [
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5547,
     "status": "ok",
     "timestamp": 1620418929903,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 240
    },
    "id": "_npZeCIqcKjT",
    "outputId": "8951acb2-32ca-4e07-e1cf-011aa6831a69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aspect': 'ambience', 'sentiment': 0, 'text': 'Certainly not the best sushi in New York, however, it is always fresh, and the place is very clean, sterile.'}\n"
     ]
    }
   ],
   "source": [
    "print(test_set[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TOMmAtIvoZ_"
   },
   "source": [
    "# Zero-shot ATSC with Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jNAtuv-hbzv"
   },
   "source": [
    "## Initialize BERT MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "En2BmfjVhbzy"
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"../bert_base_uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEIbN5Xthb0o"
   },
   "source": [
    "## Define a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12519,
     "status": "ok",
     "timestamp": 1620418936896,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 240
    },
    "id": "wN3q4Rsopxby",
    "outputId": "f9336495-e129-4ca0-d311-6bc3c2f38c20"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../bert_base_uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../bert_base_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "classifier_model = transformers.AutoModelForSequenceClassification.from_pretrained(\"../bert_base_uncased\", num_labels=3)\n",
    "\n",
    "# classifier_model = classifier_model.to(device=torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1U6B5GNSYBYk"
   },
   "source": [
    "## Load our saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14206,
     "status": "ok",
     "timestamp": 1620418938595,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 240
    },
    "id": "gLo25nUcYBGx",
    "outputId": "f5fef2db-f954-4d65-b4c9-c8e93a269b5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading epoch_0.pt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BertForSequenceClassification:\n\tMissing key(s) in state_dict: \"bert.embeddings.position_ids\", \"bert.embeddings.word_embeddings.weight\", \"bert.embeddings.position_embeddings.weight\", \"bert.embeddings.token_type_embeddings.weight\", \"bert.embeddings.LayerNorm.weight\", \"bert.embeddings.LayerNorm.bias\", \"bert.encoder.layer.0.attention.self.query.weight\", \"bert.encoder.layer.0.attention.self.query.bias\", \"bert.encoder.layer.0.attention.self.key.weight\", \"bert.encoder.layer.0.attention.self.key.bias\", \"bert.encoder.layer.0.attention.self.value.weight\", \"bert.encoder.layer.0.attention.self.value.bias\", \"bert.encoder.layer.0.attention.output.dense.weight\", \"bert.encoder.layer.0.attention.output.dense.bias\", \"bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert.encoder.layer.0.intermediate.dense.weight\", \"bert.encoder.layer.0.intermediate.dense.bias\", \"bert.encoder.layer.0.output.dense.weight\", \"bert.encoder.layer.0.output.dense.bias\", \"bert.encoder.layer.0.output.LayerNorm.weight\", \"bert.encoder.layer.0.output.LayerNorm.bias\", \"bert.encoder.layer.1.attention.self.query.weight\", \"bert.encoder.layer.1.attention.self.query.bias\", \"bert.encoder.layer.1.attention.self.key.weight\", \"bert.encoder.layer.1.attention.self.key.bias\", \"bert.encoder.layer.1.attention.self.value.weight\", \"bert.encoder.layer.1.attention.self.value.bias\", \"bert.encoder.layer.1.attention.output.dense.weight\", \"bert.encoder.layer.1.attention.output.dense.bias\", \"bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"bert.encoder.layer.1.intermediate.dense.weight\", \"bert.encoder.layer.1.intermediate.dense.bias\", \"bert.encoder.layer.1.output.dense.weight\", \"bert.encoder.layer.1.output.dense.bias\", \"bert.encoder.layer.1.output.LayerNorm.weight\", \"bert.encoder.layer.1.output.LayerNorm.bias\", \"bert.encoder.layer.2.attention.self.query.weight\", \"bert.encoder.layer.2.attention.self.query.bias\", \"bert.encoder.layer.2.attention.self.key.weight\", \"bert.encoder.layer.2.attention.self.key.bias\", \"bert.encoder.layer.2.attention.self.value.weight\", \"bert.encoder.layer.2.attention.self.value.bias\", \"bert.encoder.layer.2.attention.output.dense.weight\", \"bert.encoder.layer.2.attention.output.dense.bias\", \"bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"bert.encoder.layer.2.intermediate.dense.weight\", \"bert.encoder.layer.2.intermediate.dense.bias\", \"bert.encoder.layer.2.output.dense.weight\", \"bert.encoder.layer.2.output.dense.bias\", \"bert.encoder.layer.2.output.LayerNorm.weight\", \"bert.encoder.layer.2.output.LayerNorm.bias\", \"bert.encoder.layer.3.attention.self.query.weight\", \"bert.encoder.layer.3.attention.self.query.bias\", \"bert.encoder.layer.3.attention.self.key.weight\", \"bert.encoder.layer.3.attention.self.key.bias\", \"bert.encoder.layer.3.attention.self.value.weight\", \"bert.encoder.layer.3.attention.self.value.bias\", \"bert.encoder.layer.3.attention.output.dense.weight\", \"bert.encoder.layer.3.attention.output.dense.bias\", \"bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"bert.encoder.layer.3.intermediate.dense.weight\", \"bert.encoder.layer.3.intermediate.dense.bias\", \"bert.encoder.layer.3.output.dense.weight\", \"bert.encoder.layer.3.output.dense.bias\", \"bert.encoder.layer.3.output.LayerNorm.weight\", \"bert.encoder.layer.3.output.LayerNorm.bias\", \"bert.encoder.layer.4.attention.self.query.weight\", \"bert.encoder.layer.4.attention.self.query.bias\", \"bert.encoder.layer.4.attention.self.key.weight\", \"bert.encoder.layer.4.attention.self.key.bias\", \"bert.encoder.layer.4.attention.self.value.weight\", \"bert.encoder.layer.4.attention.self.value.bias\", \"bert.encoder.layer.4.attention.output.dense.weight\", \"bert.encoder.layer.4.attention.output.dense.bias\", \"bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"bert.encoder.layer.4.intermediate.dense.weight\", \"bert.encoder.layer.4.intermediate.dense.bias\", \"bert.encoder.layer.4.output.dense.weight\", \"bert.encoder.layer.4.output.dense.bias\", \"bert.encoder.layer.4.output.LayerNorm.weight\", \"bert.encoder.layer.4.output.LayerNorm.bias\", \"bert.encoder.layer.5.attention.self.query.weight\", \"bert.encoder.layer.5.attention.self.query.bias\", \"bert.encoder.layer.5.attention.self.key.weight\", \"bert.encoder.layer.5.attention.self.key.bias\", \"bert.encoder.layer.5.attention.self.value.weight\", \"bert.encoder.layer.5.attention.self.value.bias\", \"bert.encoder.layer.5.attention.output.dense.weight\", \"bert.encoder.layer.5.attention.output.dense.bias\", \"bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"bert.encoder.layer.5.intermediate.dense.weight\", \"bert.encoder.layer.5.intermediate.dense.bias\", \"bert.encoder.layer.5.output.dense.weight\", \"bert.encoder.layer.5.output.dense.bias\", \"bert.encoder.layer.5.output.LayerNorm.weight\", \"bert.encoder.layer.5.output.LayerNorm.bias\", \"bert.encoder.layer.6.attention.self.query.weight\", \"bert.encoder.layer.6.attention.self.query.bias\", \"bert.encoder.layer.6.attention.self.key.weight\", \"bert.encoder.layer.6.attention.self.key.bias\", \"bert.encoder.layer.6.attention.self.value.weight\", \"bert.encoder.layer.6.attention.self.value.bias\", \"bert.encoder.layer.6.attention.output.dense.weight\", \"bert.encoder.layer.6.attention.output.dense.bias\", \"bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"bert.encoder.layer.6.intermediate.dense.weight\", \"bert.encoder.layer.6.intermediate.dense.bias\", \"bert.encoder.layer.6.output.dense.weight\", \"bert.encoder.layer.6.output.dense.bias\", \"bert.encoder.layer.6.output.LayerNorm.weight\", \"bert.encoder.layer.6.output.LayerNorm.bias\", \"bert.encoder.layer.7.attention.self.query.weight\", \"bert.encoder.layer.7.attention.self.query.bias\", \"bert.encoder.layer.7.attention.self.key.weight\", \"bert.encoder.layer.7.attention.self.key.bias\", \"bert.encoder.layer.7.attention.self.value.weight\", \"bert.encoder.layer.7.attention.self.value.bias\", \"bert.encoder.layer.7.attention.output.dense.weight\", \"bert.encoder.layer.7.attention.output.dense.bias\", \"bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"bert.encoder.layer.7.intermediate.dense.weight\", \"bert.encoder.layer.7.intermediate.dense.bias\", \"bert.encoder.layer.7.output.dense.weight\", \"bert.encoder.layer.7.output.dense.bias\", \"bert.encoder.layer.7.output.LayerNorm.weight\", \"bert.encoder.layer.7.output.LayerNorm.bias\", \"bert.encoder.layer.8.attention.self.query.weight\", \"bert.encoder.layer.8.attention.self.query.bias\", \"bert.encoder.layer.8.attention.self.key.weight\", \"bert.encoder.layer.8.attention.self.key.bias\", \"bert.encoder.layer.8.attention.self.value.weight\", \"bert.encoder.layer.8.attention.self.value.bias\", \"bert.encoder.layer.8.attention.output.dense.weight\", \"bert.encoder.layer.8.attention.output.dense.bias\", \"bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"bert.encoder.layer.8.intermediate.dense.weight\", \"bert.encoder.layer.8.intermediate.dense.bias\", \"bert.encoder.layer.8.output.dense.weight\", \"bert.encoder.layer.8.output.dense.bias\", \"bert.encoder.layer.8.output.LayerNorm.weight\", \"bert.encoder.layer.8.output.LayerNorm.bias\", \"bert.encoder.layer.9.attention.self.query.weight\", \"bert.encoder.layer.9.attention.self.query.bias\", \"bert.encoder.layer.9.attention.self.key.weight\", \"bert.encoder.layer.9.attention.self.key.bias\", \"bert.encoder.layer.9.attention.self.value.weight\", \"bert.encoder.layer.9.attention.self.value.bias\", \"bert.encoder.layer.9.attention.output.dense.weight\", \"bert.encoder.layer.9.attention.output.dense.bias\", \"bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"bert.encoder.layer.9.intermediate.dense.weight\", \"bert.encoder.layer.9.intermediate.dense.bias\", \"bert.encoder.layer.9.output.dense.weight\", \"bert.encoder.layer.9.output.dense.bias\", \"bert.encoder.layer.9.output.LayerNorm.weight\", \"bert.encoder.layer.9.output.LayerNorm.bias\", \"bert.encoder.layer.10.attention.self.query.weight\", \"bert.encoder.layer.10.attention.self.query.bias\", \"bert.encoder.layer.10.attention.self.key.weight\", \"bert.encoder.layer.10.attention.self.key.bias\", \"bert.encoder.layer.10.attention.self.value.weight\", \"bert.encoder.layer.10.attention.self.value.bias\", \"bert.encoder.layer.10.attention.output.dense.weight\", \"bert.encoder.layer.10.attention.output.dense.bias\", \"bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"bert.encoder.layer.10.intermediate.dense.weight\", \"bert.encoder.layer.10.intermediate.dense.bias\", \"bert.encoder.layer.10.output.dense.weight\", \"bert.encoder.layer.10.output.dense.bias\", \"bert.encoder.layer.10.output.LayerNorm.weight\", \"bert.encoder.layer.10.output.LayerNorm.bias\", \"bert.encoder.layer.11.attention.self.query.weight\", \"bert.encoder.layer.11.attention.self.query.bias\", \"bert.encoder.layer.11.attention.self.key.weight\", \"bert.encoder.layer.11.attention.self.key.bias\", \"bert.encoder.layer.11.attention.self.value.weight\", \"bert.encoder.layer.11.attention.self.value.bias\", \"bert.encoder.layer.11.attention.output.dense.weight\", \"bert.encoder.layer.11.attention.output.dense.bias\", \"bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"bert.encoder.layer.11.intermediate.dense.weight\", \"bert.encoder.layer.11.intermediate.dense.bias\", \"bert.encoder.layer.11.output.dense.weight\", \"bert.encoder.layer.11.output.dense.bias\", \"bert.encoder.layer.11.output.LayerNorm.weight\", \"bert.encoder.layer.11.output.LayerNorm.bias\", \"bert.pooler.dense.weight\", \"bert.pooler.dense.bias\", \"classifier.weight\", \"classifier.bias\". \n\tUnexpected key(s) in state_dict: \"lm.bert.embeddings.position_ids\", \"lm.bert.embeddings.word_embeddings.weight\", \"lm.bert.embeddings.position_embeddings.weight\", \"lm.bert.embeddings.token_type_embeddings.weight\", \"lm.bert.embeddings.LayerNorm.weight\", \"lm.bert.embeddings.LayerNorm.bias\", \"lm.bert.encoder.layer.0.attention.self.query.weight\", \"lm.bert.encoder.layer.0.attention.self.query.bias\", \"lm.bert.encoder.layer.0.attention.self.key.weight\", \"lm.bert.encoder.layer.0.attention.self.key.bias\", \"lm.bert.encoder.layer.0.attention.self.value.weight\", \"lm.bert.encoder.layer.0.attention.self.value.bias\", \"lm.bert.encoder.layer.0.attention.output.dense.weight\", \"lm.bert.encoder.layer.0.attention.output.dense.bias\", \"lm.bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"lm.bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"lm.bert.encoder.layer.0.intermediate.dense.weight\", \"lm.bert.encoder.layer.0.intermediate.dense.bias\", \"lm.bert.encoder.layer.0.output.dense.weight\", \"lm.bert.encoder.layer.0.output.dense.bias\", \"lm.bert.encoder.layer.0.output.LayerNorm.weight\", \"lm.bert.encoder.layer.0.output.LayerNorm.bias\", \"lm.bert.encoder.layer.1.attention.self.query.weight\", \"lm.bert.encoder.layer.1.attention.self.query.bias\", \"lm.bert.encoder.layer.1.attention.self.key.weight\", \"lm.bert.encoder.layer.1.attention.self.key.bias\", \"lm.bert.encoder.layer.1.attention.self.value.weight\", \"lm.bert.encoder.layer.1.attention.self.value.bias\", \"lm.bert.encoder.layer.1.attention.output.dense.weight\", \"lm.bert.encoder.layer.1.attention.output.dense.bias\", \"lm.bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"lm.bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"lm.bert.encoder.layer.1.intermediate.dense.weight\", \"lm.bert.encoder.layer.1.intermediate.dense.bias\", \"lm.bert.encoder.layer.1.output.dense.weight\", \"lm.bert.encoder.layer.1.output.dense.bias\", \"lm.bert.encoder.layer.1.output.LayerNorm.weight\", \"lm.bert.encoder.layer.1.output.LayerNorm.bias\", \"lm.bert.encoder.layer.2.attention.self.query.weight\", \"lm.bert.encoder.layer.2.attention.self.query.bias\", \"lm.bert.encoder.layer.2.attention.self.key.weight\", \"lm.bert.encoder.layer.2.attention.self.key.bias\", \"lm.bert.encoder.layer.2.attention.self.value.weight\", \"lm.bert.encoder.layer.2.attention.self.value.bias\", \"lm.bert.encoder.layer.2.attention.output.dense.weight\", \"lm.bert.encoder.layer.2.attention.output.dense.bias\", \"lm.bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"lm.bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"lm.bert.encoder.layer.2.intermediate.dense.weight\", \"lm.bert.encoder.layer.2.intermediate.dense.bias\", \"lm.bert.encoder.layer.2.output.dense.weight\", \"lm.bert.encoder.layer.2.output.dense.bias\", \"lm.bert.encoder.layer.2.output.LayerNorm.weight\", \"lm.bert.encoder.layer.2.output.LayerNorm.bias\", \"lm.bert.encoder.layer.3.attention.self.query.weight\", \"lm.bert.encoder.layer.3.attention.self.query.bias\", \"lm.bert.encoder.layer.3.attention.self.key.weight\", \"lm.bert.encoder.layer.3.attention.self.key.bias\", \"lm.bert.encoder.layer.3.attention.self.value.weight\", \"lm.bert.encoder.layer.3.attention.self.value.bias\", \"lm.bert.encoder.layer.3.attention.output.dense.weight\", \"lm.bert.encoder.layer.3.attention.output.dense.bias\", \"lm.bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"lm.bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"lm.bert.encoder.layer.3.intermediate.dense.weight\", \"lm.bert.encoder.layer.3.intermediate.dense.bias\", \"lm.bert.encoder.layer.3.output.dense.weight\", \"lm.bert.encoder.layer.3.output.dense.bias\", \"lm.bert.encoder.layer.3.output.LayerNorm.weight\", \"lm.bert.encoder.layer.3.output.LayerNorm.bias\", \"lm.bert.encoder.layer.4.attention.self.query.weight\", \"lm.bert.encoder.layer.4.attention.self.query.bias\", \"lm.bert.encoder.layer.4.attention.self.key.weight\", \"lm.bert.encoder.layer.4.attention.self.key.bias\", \"lm.bert.encoder.layer.4.attention.self.value.weight\", \"lm.bert.encoder.layer.4.attention.self.value.bias\", \"lm.bert.encoder.layer.4.attention.output.dense.weight\", \"lm.bert.encoder.layer.4.attention.output.dense.bias\", \"lm.bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"lm.bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"lm.bert.encoder.layer.4.intermediate.dense.weight\", \"lm.bert.encoder.layer.4.intermediate.dense.bias\", \"lm.bert.encoder.layer.4.output.dense.weight\", \"lm.bert.encoder.layer.4.output.dense.bias\", \"lm.bert.encoder.layer.4.output.LayerNorm.weight\", \"lm.bert.encoder.layer.4.output.LayerNorm.bias\", \"lm.bert.encoder.layer.5.attention.self.query.weight\", \"lm.bert.encoder.layer.5.attention.self.query.bias\", \"lm.bert.encoder.layer.5.attention.self.key.weight\", \"lm.bert.encoder.layer.5.attention.self.key.bias\", \"lm.bert.encoder.layer.5.attention.self.value.weight\", \"lm.bert.encoder.layer.5.attention.self.value.bias\", \"lm.bert.encoder.layer.5.attention.output.dense.weight\", \"lm.bert.encoder.layer.5.attention.output.dense.bias\", \"lm.bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"lm.bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"lm.bert.encoder.layer.5.intermediate.dense.weight\", \"lm.bert.encoder.layer.5.intermediate.dense.bias\", \"lm.bert.encoder.layer.5.output.dense.weight\", \"lm.bert.encoder.layer.5.output.dense.bias\", \"lm.bert.encoder.layer.5.output.LayerNorm.weight\", \"lm.bert.encoder.layer.5.output.LayerNorm.bias\", \"lm.bert.encoder.layer.6.attention.self.query.weight\", \"lm.bert.encoder.layer.6.attention.self.query.bias\", \"lm.bert.encoder.layer.6.attention.self.key.weight\", \"lm.bert.encoder.layer.6.attention.self.key.bias\", \"lm.bert.encoder.layer.6.attention.self.value.weight\", \"lm.bert.encoder.layer.6.attention.self.value.bias\", \"lm.bert.encoder.layer.6.attention.output.dense.weight\", \"lm.bert.encoder.layer.6.attention.output.dense.bias\", \"lm.bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"lm.bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"lm.bert.encoder.layer.6.intermediate.dense.weight\", \"lm.bert.encoder.layer.6.intermediate.dense.bias\", \"lm.bert.encoder.layer.6.output.dense.weight\", \"lm.bert.encoder.layer.6.output.dense.bias\", \"lm.bert.encoder.layer.6.output.LayerNorm.weight\", \"lm.bert.encoder.layer.6.output.LayerNorm.bias\", \"lm.bert.encoder.layer.7.attention.self.query.weight\", \"lm.bert.encoder.layer.7.attention.self.query.bias\", \"lm.bert.encoder.layer.7.attention.self.key.weight\", \"lm.bert.encoder.layer.7.attention.self.key.bias\", \"lm.bert.encoder.layer.7.attention.self.value.weight\", \"lm.bert.encoder.layer.7.attention.self.value.bias\", \"lm.bert.encoder.layer.7.attention.output.dense.weight\", \"lm.bert.encoder.layer.7.attention.output.dense.bias\", \"lm.bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"lm.bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"lm.bert.encoder.layer.7.intermediate.dense.weight\", \"lm.bert.encoder.layer.7.intermediate.dense.bias\", \"lm.bert.encoder.layer.7.output.dense.weight\", \"lm.bert.encoder.layer.7.output.dense.bias\", \"lm.bert.encoder.layer.7.output.LayerNorm.weight\", \"lm.bert.encoder.layer.7.output.LayerNorm.bias\", \"lm.bert.encoder.layer.8.attention.self.query.weight\", \"lm.bert.encoder.layer.8.attention.self.query.bias\", \"lm.bert.encoder.layer.8.attention.self.key.weight\", \"lm.bert.encoder.layer.8.attention.self.key.bias\", \"lm.bert.encoder.layer.8.attention.self.value.weight\", \"lm.bert.encoder.layer.8.attention.self.value.bias\", \"lm.bert.encoder.layer.8.attention.output.dense.weight\", \"lm.bert.encoder.layer.8.attention.output.dense.bias\", \"lm.bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"lm.bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"lm.bert.encoder.layer.8.intermediate.dense.weight\", \"lm.bert.encoder.layer.8.intermediate.dense.bias\", \"lm.bert.encoder.layer.8.output.dense.weight\", \"lm.bert.encoder.layer.8.output.dense.bias\", \"lm.bert.encoder.layer.8.output.LayerNorm.weight\", \"lm.bert.encoder.layer.8.output.LayerNorm.bias\", \"lm.bert.encoder.layer.9.attention.self.query.weight\", \"lm.bert.encoder.layer.9.attention.self.query.bias\", \"lm.bert.encoder.layer.9.attention.self.key.weight\", \"lm.bert.encoder.layer.9.attention.self.key.bias\", \"lm.bert.encoder.layer.9.attention.self.value.weight\", \"lm.bert.encoder.layer.9.attention.self.value.bias\", \"lm.bert.encoder.layer.9.attention.output.dense.weight\", \"lm.bert.encoder.layer.9.attention.output.dense.bias\", \"lm.bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"lm.bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"lm.bert.encoder.layer.9.intermediate.dense.weight\", \"lm.bert.encoder.layer.9.intermediate.dense.bias\", \"lm.bert.encoder.layer.9.output.dense.weight\", \"lm.bert.encoder.layer.9.output.dense.bias\", \"lm.bert.encoder.layer.9.output.LayerNorm.weight\", \"lm.bert.encoder.layer.9.output.LayerNorm.bias\", \"lm.bert.encoder.layer.10.attention.self.query.weight\", \"lm.bert.encoder.layer.10.attention.self.query.bias\", \"lm.bert.encoder.layer.10.attention.self.key.weight\", \"lm.bert.encoder.layer.10.attention.self.key.bias\", \"lm.bert.encoder.layer.10.attention.self.value.weight\", \"lm.bert.encoder.layer.10.attention.self.value.bias\", \"lm.bert.encoder.layer.10.attention.output.dense.weight\", \"lm.bert.encoder.layer.10.attention.output.dense.bias\", \"lm.bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"lm.bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"lm.bert.encoder.layer.10.intermediate.dense.weight\", \"lm.bert.encoder.layer.10.intermediate.dense.bias\", \"lm.bert.encoder.layer.10.output.dense.weight\", \"lm.bert.encoder.layer.10.output.dense.bias\", \"lm.bert.encoder.layer.10.output.LayerNorm.weight\", \"lm.bert.encoder.layer.10.output.LayerNorm.bias\", \"lm.bert.encoder.layer.11.attention.self.query.weight\", \"lm.bert.encoder.layer.11.attention.self.query.bias\", \"lm.bert.encoder.layer.11.attention.self.key.weight\", \"lm.bert.encoder.layer.11.attention.self.key.bias\", \"lm.bert.encoder.layer.11.attention.self.value.weight\", \"lm.bert.encoder.layer.11.attention.self.value.bias\", \"lm.bert.encoder.layer.11.attention.output.dense.weight\", \"lm.bert.encoder.layer.11.attention.output.dense.bias\", \"lm.bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"lm.bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"lm.bert.encoder.layer.11.intermediate.dense.weight\", \"lm.bert.encoder.layer.11.intermediate.dense.bias\", \"lm.bert.encoder.layer.11.output.dense.weight\", \"lm.bert.encoder.layer.11.output.dense.bias\", \"lm.bert.encoder.layer.11.output.LayerNorm.weight\", \"lm.bert.encoder.layer.11.output.LayerNorm.bias\", \"lm.cls.predictions.bias\", \"lm.cls.predictions.transform.dense.weight\", \"lm.cls.predictions.transform.dense.bias\", \"lm.cls.predictions.transform.LayerNorm.weight\", \"lm.cls.predictions.transform.LayerNorm.bias\", \"lm.cls.predictions.decoder.weight\", \"lm.cls.predictions.decoder.bias\", \"linear.weight\", \"linear.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1356/4273636647.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_weights_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m classifier_model.load_state_dict(torch.load(\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_weights_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     map_location=torch_device))\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1052\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BertForSequenceClassification:\n\tMissing key(s) in state_dict: \"bert.embeddings.position_ids\", \"bert.embeddings.word_embeddings.weight\", \"bert.embeddings.position_embeddings.weight\", \"bert.embeddings.token_type_embeddings.weight\", \"bert.embeddings.LayerNorm.weight\", \"bert.embeddings.LayerNorm.bias\", \"bert.encoder.layer.0.attention.self.query.weight\", \"bert.encoder.layer.0.attention.self.query.bias\", \"bert.encoder.layer.0.attention.self.key.weight\", \"bert.encoder.layer.0.attention.self.key.bias\", \"bert.encoder.layer.0.attention.self.value.weight\", \"bert.encoder.layer.0.attention.self.value.bias\", \"bert.encoder.layer.0.attention.output.dense.weight\", \"bert.encoder.layer.0.attention.output.dense.bias\", \"bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert.encoder.layer.0.intermediate.dense.weight\", \"bert.encoder.layer.0.intermediate.dense.bias\", \"bert.encoder.layer.0.output.dense.weight\", \"bert.encoder.layer.0.output.dense.bias\", \"bert.encoder.layer.0.output.LayerNorm.weight\", \"bert.encoder.layer.0.output.LayerNorm.bias\", \"bert.encoder.layer.1.attention.self.query.weight\", \"bert.encoder.layer.1.attention.self.query.bias\", \"bert.encoder.layer.1.attention.self.key.weight\", \"bert.encoder.layer.1.attention.self.key.bias\", \"bert.encoder.layer.1.attention.self.value.weight\", \"bert.encoder.layer.1.attention.self.value.bias\", \"bert.encoder.layer.1.attention.output.dense.weight\", \"bert.encoder.layer.1.attention.output.dense.bias\", \"bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"bert.encoder.layer.1.intermediate.dense.weight\", \"bert.encoder.layer.1.intermediate.dense.bias\", \"bert.encoder.layer.1.output.dense.weight\", \"bert.encoder.layer.1.output.dense.bias\", \"bert.encoder.layer.1.output.LayerNorm.weight\", \"bert.encoder.layer.1.output.LayerNorm.bias\", \"bert.encoder.layer.2.attention.self.query.weight\", \"bert.encoder.layer.2.attention.self.query.bias\", \"bert.encoder.layer.2.attention.self.key.weight\", \"bert.encoder.layer.2.attention.self.key.bias\", \"bert.encoder.layer.2.attention.self.value.weight\", \"bert.encoder.layer.2.attention.self.value.bias\", \"bert.encoder.layer.2.attention.output.dense.weight\", \"bert.encoder.layer.2.attention.output.dense.bias\", \"bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"bert.encoder.layer.2.intermediate.dense.weight\", \"bert.encoder.layer.2.intermediate.dense.bias\", \"bert.encoder.layer.2.output.dense.weight\", \"bert.encoder.layer.2.output.dense.bias\", \"bert.encoder.layer.2.output.LayerNorm.weight\", \"bert.encoder.layer.2.output.LayerNorm.bias\", \"bert.encoder.layer.3.attention.self.query.weight\", \"bert.encoder.layer.3.attention.self.query.bias\", \"bert.encoder.layer.3.attention.self.key.weight\", \"bert.encoder.layer.3.attention.self.key.bias\", \"bert.encoder.layer.3.attention.self.value.weight\", \"bert.encoder.layer.3.attention.self.value.bias\", \"bert.encoder.layer.3.attention.output.dense.weight\", \"bert.encoder.layer.3.attention.output.dense.bias\", \"bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"bert.encoder.layer.3.intermediate.dense.weight\", \"bert.encoder.layer.3.intermediate.dense.bias\", \"bert.encoder.layer.3.output.dense.weight\", \"bert.encoder.layer.3.output.dense.bias\", \"bert.encoder.layer.3.output.LayerNorm.weight\", \"bert.encoder.layer.3.output.LayerNorm.bias\", \"bert.encoder.layer.4.attention.self.query.weight\", \"bert.encoder.layer.4.attention.self.query.bias\", \"bert.encoder.layer.4.attention.self.key.weight\", \"bert.encoder.layer.4.attention.self.key.bias\", \"bert.encoder.layer.4.attention.self.value.weight\", \"bert.encoder.layer.4.attention.self.value.bias\", \"bert.encoder.layer.4.attention.output.dense.weight\", \"bert.encoder.layer.4.attention.output.dense.bias\", \"bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"bert.encoder.layer.4.intermediate.dense.weight\", \"bert.encoder.layer.4.intermediate.dense.bias\", \"bert.encoder.layer.4.output.dense.weight\", \"bert.encoder.layer.4.output.dense.bias\", \"bert.encoder.layer.4.output.LayerNorm.weight\", \"bert.encoder.layer.4.output.LayerNorm.bias\", \"bert.encoder.layer.5.attention.self.query.weight\", \"bert.encoder.layer.5.attention.self.query.bias\", \"bert.encoder.layer.5.attention.self.key.weight\", \"bert.encoder.layer.5.attention.self.key.bias\", \"bert.encoder.layer.5.attention.self.value.weight\", \"bert.encoder.layer.5.attention.self.value.bias\", \"bert.encoder.layer.5.attention.output.dense.weight\", \"bert.encoder.layer.5.attention.output.dense.bias\", \"bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"bert.encoder.layer.5.intermediate.dense.weight\", \"bert.encoder.layer.5.intermediate.dense.bias\", \"bert.encoder.layer.5.output.dense.weight\", \"bert.encoder.layer.5.output.dense.bias\", \"bert.encoder.layer.5.output.LayerNorm.weight\", \"bert.encoder.layer.5.output.LayerNorm.bias\", \"bert.encoder.layer.6.attention.self.query.weight\", \"bert.encoder.layer.6.attention.self.query.bias\", \"bert.encoder.layer.6.attention.self.key.weight\", \"bert.encoder.layer.6.attention.self.key.bias\", \"bert.encoder.layer.6.attention.self.value.weight\", \"bert.encoder.layer.6.attention.self.value.bias\", \"bert.encoder.layer.6.attention.output.dense.weight\", \"bert.encoder.layer.6.attention.output.dense.bias\", \"bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"bert.encoder.layer.6.intermediate.dense.weight\", \"bert.encoder.layer.6.intermediate.dense.bias\", \"bert.encoder.layer.6.output.dense.weight\", \"bert.encoder.layer.6.output.dense.bias\", \"bert.encoder.layer.6.output.LayerNorm.weight\", \"bert.encoder.layer.6.output.LayerNorm.bias\", \"bert.encoder.layer.7.attention.self.query.weight\", \"bert.encoder.layer.7.attention.self.query.bias\", \"bert.encoder.layer.7.attention.self.key.weight\", \"bert.encoder.layer.7.attention.self.key.bias\", \"bert.encoder.layer.7.attention.self.value.weight\", \"bert.encoder.layer.7.attention.self.value.bias\", \"bert.encoder.layer.7.attention.output.dense.weight\", \"bert.encoder.layer.7.attention.output.dense.bias\", \"bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"bert.encoder.layer.7.intermediate.dense.weight\", \"bert.encoder.layer.7.intermediate.dense.bias\", \"bert.encoder.layer.7.output.dense.weight\", \"bert.encoder.layer.7.output.dense.bias\", \"bert.encoder.layer.7.output.LayerNorm.weight\", \"bert.encoder.layer.7.output.LayerNorm.bias\", \"bert.encoder.layer.8.attention.self.query.weight\", \"bert.encoder.layer.8.attention.self.query.bias\", \"bert.encoder.layer.8.attention.self.key.weight\", \"bert.encoder.layer.8.attention.self.key.bias\", \"bert.encoder.layer.8.attention.self.value.weight\", \"bert.encoder.layer.8.attention.self.value.bias\", \"bert.encoder.layer.8.attention.output.dense.weight\", \"bert.encoder.layer.8.attention.output.dense.bias\", \"bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"bert.encoder.layer.8.intermediate.dense.weight\", \"bert.encoder.layer.8.intermediate.dense.bias\", \"bert.encoder.layer.8.output.dense.weight\", \"bert.encoder.layer.8.output.dense.bias\", \"bert.encoder.layer.8.output.LayerNorm.weight\", \"bert.encoder.layer.8.output.LayerNorm.bias\", \"bert.encoder.layer.9.attention.self.query.weight\", \"bert.encoder.layer.9.attention.self.query.bias\", \"bert.encoder.layer.9.attention.self.key.weight\", \"bert.encoder.layer.9.attention.self.key.bias\", \"bert.encoder.layer.9.attention.self.value.weight\", \"bert.encoder.layer.9.attention.self.value.bias\", \"bert.encoder.layer.9.attention.output.dense.weight\", \"bert.encoder.layer.9.attention.output.dense.bias\", \"bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"bert.encoder.layer.9.intermediate.dense.weight\", \"bert.encoder.layer.9.intermediate.dense.bias\", \"bert.encoder.layer.9.output.dense.weight\", \"bert.encoder.layer.9.output.dense.bias\", \"bert.encoder.layer.9.output.LayerNorm.weight\", \"bert.encoder.layer.9.output.LayerNorm.bias\", \"bert.encoder.layer.10.attention.self.query.weight\", \"bert.encoder.layer.10.attention.self.query.bias\", \"bert.encoder.layer.10.attention.self.key.weight\", \"bert.encoder.layer.10.attention.self.key.bias\", \"bert.encoder.layer.10.attention.self.value.weight\", \"bert.encoder.layer.10.attention.self.value.bias\", \"bert.encoder.layer.10.attention.output.dense.weight\", \"bert.encoder.layer.10.attention.output.dense.bias\", \"bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"bert.encoder.layer.10.intermediate.dense.weight\", \"bert.encoder.layer.10.intermediate.dense.bias\", \"bert.encoder.layer.10.output.dense.weight\", \"bert.encoder.layer.10.output.dense.bias\", \"bert.encoder.layer.10.output.LayerNorm.weight\", \"bert.encoder.layer.10.output.LayerNorm.bias\", \"bert.encoder.layer.11.attention.self.query.weight\", \"bert.encoder.layer.11.attention.self.query.bias\", \"bert.encoder.layer.11.attention.self.key.weight\", \"bert.encoder.layer.11.attention.self.key.bias\", \"bert.encoder.layer.11.attention.self.value.weight\", \"bert.encoder.layer.11.attention.self.value.bias\", \"bert.encoder.layer.11.attention.output.dense.weight\", \"bert.encoder.layer.11.attention.output.dense.bias\", \"bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"bert.encoder.layer.11.intermediate.dense.weight\", \"bert.encoder.layer.11.intermediate.dense.bias\", \"bert.encoder.layer.11.output.dense.weight\", \"bert.encoder.layer.11.output.dense.bias\", \"bert.encoder.layer.11.output.LayerNorm.weight\", \"bert.encoder.layer.11.output.LayerNorm.bias\", \"bert.pooler.dense.weight\", \"bert.pooler.dense.bias\", \"classifier.weight\", \"classifier.bias\". \n\tUnexpected key(s) in state_dict: \"lm.bert.embeddings.position_ids\", \"lm.bert.embeddings.word_embeddings.weight\", \"lm.bert.embeddings.position_embeddings.weight\", \"lm.bert.embeddings.token_type_embeddings.weight\", \"lm.bert.embeddings.LayerNorm.weight\", \"lm.bert.embeddings.LayerNorm.bias\", \"lm.bert.encoder.layer.0.attention.self.query.weight\", \"lm.bert.encoder.layer.0.attention.self.query.bias\", \"lm.bert.encoder.layer.0.attention.self.key.weight\", \"lm.bert.encoder.layer.0.attention.self.key.bias\", \"lm.bert.encoder.layer.0.attention.self.value.weight\", \"lm.bert.encoder.layer.0.attention.self.value.bias\", \"lm.bert.encoder.layer.0.attention.output.dense.weight\", \"lm.bert.encoder.layer.0.attention.output.dense.bias\", \"lm.bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"lm.bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"lm.bert.encoder.layer.0.intermediate.dense.weight\", \"lm.bert.encoder.layer.0.intermediate.dense.bias\", \"lm.bert.encoder.layer.0.output.dense.weight\", \"lm.bert.encoder.layer.0.output.dense.bias\", \"lm.bert.encoder.layer.0.output.LayerNorm.weight\", \"lm.bert.encoder.layer.0.output.LayerNorm.bias\", \"lm.bert.encoder.layer.1.attention.self.query.weight\", \"lm.bert.encoder.layer.1.attention.self.query.bias\", \"lm.bert.encoder.layer.1.attention.self.key.weight\", \"lm.bert.encoder.layer.1.attention.self.key.bias\", \"lm.bert.encoder.layer.1.attention.self.value.weight\", \"lm.bert.encoder.layer.1.attention.self.value.bias\", \"lm.bert.encoder.layer.1.attention.output.dense.weight\", \"lm.bert.encoder.layer.1.attention.output.dense.bias\", \"lm.bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"lm.bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"lm.bert.encoder.layer.1.intermediate.dense.weight\", \"lm.bert.encoder.layer.1.intermediate.dense.bias\", \"lm.bert.encoder.layer.1.output.dense.weight\", \"lm.bert.encoder.layer.1.output.dense.bias\", \"lm.bert.encoder.layer.1.output.LayerNorm.weight\", \"lm.bert.encoder.layer.1.output.LayerNorm.bias\", \"lm.bert.encoder.layer.2.attention.self.query.weight\", \"lm.bert.encoder.layer.2.attention.self.query.bias\", \"lm.bert.encoder.layer.2.attention.self.key.weight\", \"lm.bert.encoder.layer.2.attention.self.key.bias\", \"lm.bert.encoder.layer.2.attention.self.value.weight\", \"lm.bert.encoder.layer.2.attention.self.value.bias\", \"lm.bert.encoder.layer.2.attention.output.dense.weight\", \"lm.bert.encoder.layer.2.attention.output.dense.bias\", \"lm.bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"lm.bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"lm.bert.encoder.layer.2.intermediate.dense.weight\", \"lm.bert.encoder.layer.2.intermediate.dense.bias\", \"lm.bert.encoder.layer.2.output.dense.weight\", \"lm.bert.encoder.layer.2.output.dense.bias\", \"lm.bert.encoder.layer.2.output.LayerNorm.weight\", \"lm.bert.encoder.layer.2.output.LayerNorm.bias\", \"lm.bert.encoder.layer.3.attention.self.query.weight\", \"lm.bert.encoder.layer.3.attention.self.query.bias\", \"lm.bert.encoder.layer.3.attention.self.key.weight\", \"lm.bert.encoder.layer.3.attention.self.key.bias\", \"lm.bert.encoder.layer.3.attention.self.value.weight\", \"lm.bert.encoder.layer.3.attention.self.value.bias\", \"lm.bert.encoder.layer.3.attention.output.dense.weight\", \"lm.bert.encoder.layer.3.attention.output.dense.bias\", \"lm.bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"lm.bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"lm.bert.encoder.layer.3.intermediate.dense.weight\", \"lm.bert.encoder.layer.3.intermediate.dense.bias\", \"lm.bert.encoder.layer.3.output.dense.weight\", \"lm.bert.encoder.layer.3.output.dense.bias\", \"lm.bert.encoder.layer.3.output.LayerNorm.weight\", \"lm.bert.encoder.layer.3.output.LayerNorm.bias\", \"lm.bert.encoder.layer.4.attention.self.query.weight\", \"lm.bert.encoder.layer.4.attention.self.query.bias\", \"lm.bert.encoder.layer.4.attention.self.key.weight\", \"lm.bert.encoder.layer.4.attention.self.key.bias\", \"lm.bert.encoder.layer.4.attention.self.value.weight\", \"lm.bert.encoder.layer.4.attention.self.value.bias\", \"lm.bert.encoder.layer.4.attention.output.dense.weight\", \"lm.bert.encoder.layer.4.attention.output.dense.bias\", \"lm.bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"lm.bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"lm.bert.encoder.layer.4.intermediate.dense.weight\", \"lm.bert.encoder.layer.4.intermediate.dense.bias\", \"lm.bert.encoder.layer.4.output.dense.weight\", \"lm.bert.encoder.layer.4.output.dense.bias\", \"lm.bert.encoder.layer.4.output.LayerNorm.weight\", \"lm.bert.encoder.layer.4.output.LayerNorm.bias\", \"lm.bert.encoder.layer.5.attention.self.query.weight\", \"lm.bert.encoder.layer.5.attention.self.query.bias\", \"lm.bert.encoder.layer.5.attention.self.key.weight\", \"lm.bert.encoder.layer.5.attention.self.key.bias\", \"lm.bert.encoder.layer.5.attention.self.value.weight\", \"lm.bert.encoder.layer.5.attention.self.value.bias\", \"lm.bert.encoder.layer.5.attention.output.dense.weight\", \"lm.bert.encoder.layer.5.attention.output.dense.bias\", \"lm.bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"lm.bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"lm.bert.encoder.layer.5.intermediate.dense.weight\", \"lm.bert.encoder.layer.5.intermediate.dense.bias\", \"lm.bert.encoder.layer.5.output.dense.weight\", \"lm.bert.encoder.layer.5.output.dense.bias\", \"lm.bert.encoder.layer.5.output.LayerNorm.weight\", \"lm.bert.encoder.layer.5.output.LayerNorm.bias\", \"lm.bert.encoder.layer.6.attention.self.query.weight\", \"lm.bert.encoder.layer.6.attention.self.query.bias\", \"lm.bert.encoder.layer.6.attention.self.key.weight\", \"lm.bert.encoder.layer.6.attention.self.key.bias\", \"lm.bert.encoder.layer.6.attention.self.value.weight\", \"lm.bert.encoder.layer.6.attention.self.value.bias\", \"lm.bert.encoder.layer.6.attention.output.dense.weight\", \"lm.bert.encoder.layer.6.attention.output.dense.bias\", \"lm.bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"lm.bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"lm.bert.encoder.layer.6.intermediate.dense.weight\", \"lm.bert.encoder.layer.6.intermediate.dense.bias\", \"lm.bert.encoder.layer.6.output.dense.weight\", \"lm.bert.encoder.layer.6.output.dense.bias\", \"lm.bert.encoder.layer.6.output.LayerNorm.weight\", \"lm.bert.encoder.layer.6.output.LayerNorm.bias\", \"lm.bert.encoder.layer.7.attention.self.query.weight\", \"lm.bert.encoder.layer.7.attention.self.query.bias\", \"lm.bert.encoder.layer.7.attention.self.key.weight\", \"lm.bert.encoder.layer.7.attention.self.key.bias\", \"lm.bert.encoder.layer.7.attention.self.value.weight\", \"lm.bert.encoder.layer.7.attention.self.value.bias\", \"lm.bert.encoder.layer.7.attention.output.dense.weight\", \"lm.bert.encoder.layer.7.attention.output.dense.bias\", \"lm.bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"lm.bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"lm.bert.encoder.layer.7.intermediate.dense.weight\", \"lm.bert.encoder.layer.7.intermediate.dense.bias\", \"lm.bert.encoder.layer.7.output.dense.weight\", \"lm.bert.encoder.layer.7.output.dense.bias\", \"lm.bert.encoder.layer.7.output.LayerNorm.weight\", \"lm.bert.encoder.layer.7.output.LayerNorm.bias\", \"lm.bert.encoder.layer.8.attention.self.query.weight\", \"lm.bert.encoder.layer.8.attention.self.query.bias\", \"lm.bert.encoder.layer.8.attention.self.key.weight\", \"lm.bert.encoder.layer.8.attention.self.key.bias\", \"lm.bert.encoder.layer.8.attention.self.value.weight\", \"lm.bert.encoder.layer.8.attention.self.value.bias\", \"lm.bert.encoder.layer.8.attention.output.dense.weight\", \"lm.bert.encoder.layer.8.attention.output.dense.bias\", \"lm.bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"lm.bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"lm.bert.encoder.layer.8.intermediate.dense.weight\", \"lm.bert.encoder.layer.8.intermediate.dense.bias\", \"lm.bert.encoder.layer.8.output.dense.weight\", \"lm.bert.encoder.layer.8.output.dense.bias\", \"lm.bert.encoder.layer.8.output.LayerNorm.weight\", \"lm.bert.encoder.layer.8.output.LayerNorm.bias\", \"lm.bert.encoder.layer.9.attention.self.query.weight\", \"lm.bert.encoder.layer.9.attention.self.query.bias\", \"lm.bert.encoder.layer.9.attention.self.key.weight\", \"lm.bert.encoder.layer.9.attention.self.key.bias\", \"lm.bert.encoder.layer.9.attention.self.value.weight\", \"lm.bert.encoder.layer.9.attention.self.value.bias\", \"lm.bert.encoder.layer.9.attention.output.dense.weight\", \"lm.bert.encoder.layer.9.attention.output.dense.bias\", \"lm.bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"lm.bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"lm.bert.encoder.layer.9.intermediate.dense.weight\", \"lm.bert.encoder.layer.9.intermediate.dense.bias\", \"lm.bert.encoder.layer.9.output.dense.weight\", \"lm.bert.encoder.layer.9.output.dense.bias\", \"lm.bert.encoder.layer.9.output.LayerNorm.weight\", \"lm.bert.encoder.layer.9.output.LayerNorm.bias\", \"lm.bert.encoder.layer.10.attention.self.query.weight\", \"lm.bert.encoder.layer.10.attention.self.query.bias\", \"lm.bert.encoder.layer.10.attention.self.key.weight\", \"lm.bert.encoder.layer.10.attention.self.key.bias\", \"lm.bert.encoder.layer.10.attention.self.value.weight\", \"lm.bert.encoder.layer.10.attention.self.value.bias\", \"lm.bert.encoder.layer.10.attention.output.dense.weight\", \"lm.bert.encoder.layer.10.attention.output.dense.bias\", \"lm.bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"lm.bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"lm.bert.encoder.layer.10.intermediate.dense.weight\", \"lm.bert.encoder.layer.10.intermediate.dense.bias\", \"lm.bert.encoder.layer.10.output.dense.weight\", \"lm.bert.encoder.layer.10.output.dense.bias\", \"lm.bert.encoder.layer.10.output.LayerNorm.weight\", \"lm.bert.encoder.layer.10.output.LayerNorm.bias\", \"lm.bert.encoder.layer.11.attention.self.query.weight\", \"lm.bert.encoder.layer.11.attention.self.query.bias\", \"lm.bert.encoder.layer.11.attention.self.key.weight\", \"lm.bert.encoder.layer.11.attention.self.key.bias\", \"lm.bert.encoder.layer.11.attention.self.value.weight\", \"lm.bert.encoder.layer.11.attention.self.value.bias\", \"lm.bert.encoder.layer.11.attention.output.dense.weight\", \"lm.bert.encoder.layer.11.attention.output.dense.bias\", \"lm.bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"lm.bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"lm.bert.encoder.layer.11.intermediate.dense.weight\", \"lm.bert.encoder.layer.11.intermediate.dense.bias\", \"lm.bert.encoder.layer.11.output.dense.weight\", \"lm.bert.encoder.layer.11.output.dense.bias\", \"lm.bert.encoder.layer.11.output.LayerNorm.weight\", \"lm.bert.encoder.layer.11.output.LayerNorm.bias\", \"lm.cls.predictions.bias\", \"lm.cls.predictions.transform.dense.weight\", \"lm.cls.predictions.transform.dense.bias\", \"lm.cls.predictions.transform.LayerNorm.weight\", \"lm.cls.predictions.transform.LayerNorm.bias\", \"lm.cls.predictions.decoder.weight\", \"lm.cls.predictions.decoder.bias\", \"linear.weight\", \"linear.bias\". "
     ]
    }
   ],
   "source": [
    "# Locate the weight file.\n",
    "trained_model_directory = os.path.join('..', 'trained_models', experiment_id)\n",
    "\n",
    "saved_weights_name = ''\n",
    "\n",
    "for fname in os.listdir(trained_model_directory):\n",
    "    if fname.startswith('epoch'):\n",
    "        saved_weights_name = fname\n",
    "        break\n",
    "\n",
    "print(\"Loading\", saved_weights_name)\n",
    "\n",
    "classifier_model.load_state_dict(torch.load(\n",
    "    os.path.join(trained_model_directory, saved_weights_name),\n",
    "    map_location=torch_device))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1l1H_XIPhb0y"
   },
   "source": [
    "## Evaluation with in-domain test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0S80DoYrqApi"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(predictions, labels):\n",
    "    preds = predictions.argmax(-1)\n",
    "\n",
    "    precision, recall, f1, _ = sklearn.metrics.precision_recall_fscore_support(\n",
    "        y_true=labels, y_pred=preds, labels=[0,1,2], average='macro')\n",
    "\n",
    "    acc = sklearn.metrics.accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9NXoBTs5h2eO"
   },
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=testing_batch_size, pin_memory=use_pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "t-rIWariaiOg"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc06f526f7e4f88ae774fcf627d6401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=31.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'size'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1356/369991909.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mbatch_test_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mbatch_test_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_test_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mbatch_test_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_test_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1494\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1497\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You cannot specify both input_ids and inputs_embeds at the same time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the best found head weights\n",
    "with torch.no_grad():\n",
    "    classifier_model.eval()\n",
    "\n",
    "    predictions_test = torch.Tensor()\n",
    "\n",
    "    labels_test = torch.Tensor()\n",
    "\n",
    "    for batch_test in tqdm.notebook.tqdm(test_dataloader):\n",
    "        batch_test_encoded = tokenizer(\n",
    "            batch_test[\"text\"], batch_test[\"aspect\"],\n",
    "            padding='max_length', truncation='only_first', max_length=256,\n",
    "            return_tensors='pt')\n",
    "\n",
    "        batch_test_encoded.to(torch_device)\n",
    "\n",
    "        batch_test_label = batch_test[\"sentiment\"]\n",
    "\n",
    "        batch_test_output = classifier_model(batch_test_encoded)\n",
    "\n",
    "        batch_test_output = batch_test_output.to('cpu')\n",
    "\n",
    "        predictions_test = torch.cat([predictions_test, batch_test_output])\n",
    "        labels_test = torch.cat([labels_test, batch_test_label])\n",
    "\n",
    "    # Compute metrics\n",
    "    test_metrics = compute_metrics(predictions_test, labels_test)\n",
    "\n",
    "    print(test_metrics)\n",
    "    \n",
    "    # Save test_metrics into a file for later processing\n",
    "    # with open(os.path.join(trained_model_directory, 'test_metrics_subtask4.json'), 'w') as test_metrics_json:\n",
    "    #     json.dump(test_metrics, test_metrics_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjpA_0m1hb08"
   },
   "source": [
    "## Results visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "executionInfo": {
     "elapsed": 95048,
     "status": "ok",
     "timestamp": 1620419019476,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 240
    },
    "id": "w9G9AUeQhb09",
    "outputId": "06eb5449-2881-43c4-98b5-ad2cee1f476f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate metrics and confusion matrix based upon predictions and true labels\n",
    "cm = sklearn.metrics.confusion_matrix(labels_test.cpu().detach().numpy(), predictions_test.detach().numpy().argmax(-1))\n",
    "\n",
    "df_cm = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[i for i in [\"positive\", \"negative\", \"neutral\"]],\n",
    "    columns=[i for i in [\"positive\", \"negative\", \"neutral\"]])\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "ax = sn.heatmap(df_cm, annot=True)\n",
    "\n",
    "ax.set(xlabel='Predicted Label', ylabel='True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Wo_Yk0LY37d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "bert_no_prompt_pooled_lr_subtask4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
